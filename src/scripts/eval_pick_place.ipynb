{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, dirname, abspath\n",
    "import sys\n",
    "CURRENT_DIR = os.getcwd()\n",
    "sys.path.insert(0, join(CURRENT_DIR, '../..'))  # Import local models\n",
    "\n",
    "from cliport.environments.environment import Environment\n",
    "import torch\n",
    "from models.PickModel import PickModel\n",
    "from models.PlaceModel import PlaceModel\n",
    "from cliport import tasks\n",
    "from cliport.dataset import RavensDataset\n",
    "import numpy as np\n",
    "from cliport.utils import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_cfg = {\n",
    "  \"save_video\": False,\n",
    "  \"save_video_path\": \"/home/ubuntu/VLM/videos/\",\n",
    "  \"add_text\": True,\n",
    "  \"fps\": 20,\n",
    "  \"video_height\": 640,\n",
    "  \"video_width\": 720,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_root = \"/home/ubuntu/cliport/cliport/environments/assets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text argument:/home/ubuntu/cliport/cliport/environments/assets/\n",
      "int args: ["
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    assets_root,\n",
    "    disp=False,\n",
    "    shared_memory=False,\n",
    "    hz=480,\n",
    "    record_cfg=record_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_model = PickModel(num_rotations=1, batchnorm = False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_model.load_state_dict(torch.load(\"/home/ubuntu/VLM/checkpoint/checkpoint_model_best_pick.pth\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_model = PlaceModel(num_rotations=12, crop_size=64, batchnorm = False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_model.load_state_dict(torch.load(\"/home/ubuntu/VLM/checkpoint/checkpoint_model_best_place.pth\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cfg = {\"dataset\":{\"type\": \"single\",\n",
    "                    \"images\": True,\n",
    "                    \"cache\": False,\n",
    "                    \"augment\":{\"theta_sigma\":60},\n",
    "                    \"cache_size\": 350},\n",
    "                    }\n",
    "\n",
    "# load data\n",
    "train_dataset = RavensDataset('/home/ubuntu/cliport/data/stack-block-pyramid-seq-seen-colors-val', train_dataset_cfg, n_demos=100, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pick(inp):\n",
    "    pick_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = 'cuda'\n",
    "        img_cuda = torch.Tensor(inp['inp_img']).to(device)\n",
    "        language_cuda = inp['lang_goal']\n",
    "\n",
    "        affordances = pick_model(img_cuda, language_cuda)\n",
    "        pick_model(img_cuda, language_cuda)\n",
    "        affordances = affordances.view(affordances.shape[0], -1)\n",
    "        preds = torch.nn.functional.softmax(affordances, dim=1)\n",
    "        preds = preds.cpu()\n",
    "        preds = preds.view(320,160)\n",
    "        location = np.unravel_index(torch.argmax(preds).numpy(), (320,160))\n",
    "    \n",
    "    return preds, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_place(inp, p0):\n",
    "    place_model.eval()\n",
    "    with torch.no_grad():\n",
    "        device = 'cuda'\n",
    "        img_cuda = torch.Tensor(inp['inp_img']).to(device)\n",
    "        language_cuda = inp['lang_goal']\n",
    "\n",
    "        affordances = place_model(img_cuda, language_cuda, p0)\n",
    "        affordances = affordances.view(affordances.shape[0], -1)\n",
    "        preds = torch.nn.functional.softmax(affordances, dim=1)\n",
    "        preds = preds.cpu()\n",
    "        preds = preds.view(12, 320,160)\n",
    "        location = np.unravel_index(torch.argmax(preds).numpy(), (12, 320,160))        \n",
    "        \n",
    "    return preds, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang Goal: put the brown block on the lightest brown block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libx264 @ 0x64bb000] -qscale is ignored, -crf is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the brown block on the lightest brown block\n",
      "Lang Goal: put the red block on the lightest brown block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libx264 @ 0x638a000] -qscale is ignored, -crf is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang Goal: put the red block on the lightest brown block\n",
      "Lang Goal: put the red block on the lightest brown block\n",
      "Lang Goal: put the red block on the lightest brown block\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 44\u001b[0m\n\u001b[1;32m     35\u001b[0m act \u001b[39m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpose0\u001b[39m\u001b[39m'\u001b[39m: (np\u001b[39m.\u001b[39masarray(p0_xyz), np\u001b[39m.\u001b[39masarray(p0_xyzw)),\n\u001b[1;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpose1\u001b[39m\u001b[39m'\u001b[39m: (np\u001b[39m.\u001b[39masarray(p1_xyz), np\u001b[39m.\u001b[39masarray(p1_xyzw)),\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpick\u001b[39m\u001b[39m'\u001b[39m: [p0_pix[\u001b[39m0\u001b[39m], p0_pix[\u001b[39m1\u001b[39m], p0_theta],\n\u001b[1;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mplace\u001b[39m\u001b[39m'\u001b[39m: [p1_pix[\u001b[39m0\u001b[39m], p1_pix[\u001b[39m1\u001b[39m], p1_theta],\n\u001b[1;32m     40\u001b[0m }\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLang Goal: \u001b[39m\u001b[39m{\u001b[39;00mlang_goal\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(act)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m     47\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:208\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Execute action with specified primitive.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m  (obs, reward, done, info) tuple containing MDP step data.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask\u001b[39m.\u001b[39;49mprimitive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmovej, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmovep, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mee, action[\u001b[39m'\u001b[39;49m\u001b[39mpose0\u001b[39;49m\u001b[39m'\u001b[39;49m], action[\u001b[39m'\u001b[39;49m\u001b[39mpose1\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    210\u001b[0m     \u001b[39m# Exit early if action times out. We still return an observation\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# so that we don't break the Gym API contract.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/cliport/cliport/tasks/primitives.py:64\u001b[0m, in \u001b[0;36mPickPlace.__call__\u001b[0;34m(self, movej, movep, ee, pose0, pose1)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     ee\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m---> 64\u001b[0m     timeout \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m movep(postplace_pose)\n\u001b[1;32m     66\u001b[0m \u001b[39m# Move to prepick pose if pick is not successful.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     ee\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:439\u001b[0m, in \u001b[0;36mEnvironment.movep\u001b[0;34m(self, pose, speed)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m\"\"\"Move UR5 to target end effector pose.\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m targj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolve_ik(pose)\n\u001b[0;32m--> 439\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmovej(targj, speed)\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:364\u001b[0m, in \u001b[0;36mEnvironment.movej\u001b[0;34m(self, targj, speed, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m     p\u001b[39m.\u001b[39msetJointMotorControlArray(\n\u001b[1;32m    358\u001b[0m         bodyIndex\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mur5,\n\u001b[1;32m    359\u001b[0m         jointIndices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoints,\n\u001b[1;32m    360\u001b[0m         controlMode\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mPOSITION_CONTROL,\n\u001b[1;32m    361\u001b[0m         targetPositions\u001b[39m=\u001b[39mstepj,\n\u001b[1;32m    362\u001b[0m         positionGains\u001b[39m=\u001b[39mgains)\n\u001b[1;32m    363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_simulation()\n\u001b[1;32m    366\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWarning: movej exceeded \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout. Skipping.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:240\u001b[0m, in \u001b[0;36mEnvironment.step_simulation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_video \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_video_frame()\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:400\u001b[0m, in \u001b[0;36mEnvironment.add_video_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_cams[\u001b[39m0\u001b[39m]\n\u001b[1;32m    399\u001b[0m image_size \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_cfg[\u001b[39m'\u001b[39m\u001b[39mvideo_height\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_cfg[\u001b[39m'\u001b[39m\u001b[39mvideo_width\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 400\u001b[0m color, depth, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_camera(config, image_size, shadow\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    401\u001b[0m color \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(color)\n\u001b[1;32m    403\u001b[0m \u001b[39m# Add language instruction to video.\u001b[39;00m\n",
      "File \u001b[0;32m~/cliport/cliport/environments/environment.py:274\u001b[0m, in \u001b[0;36mEnvironment.render_camera\u001b[0;34m(self, config, image_size, shadow)\u001b[0m\n\u001b[1;32m    271\u001b[0m projm \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mcomputeProjectionMatrixFOV(fovh, aspect_ratio, znear, zfar)\n\u001b[1;32m    273\u001b[0m \u001b[39m# Render with OpenGL camera settings.\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m _, _, color, depth, segm \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mgetCameraImage(\n\u001b[1;32m    275\u001b[0m     width\u001b[39m=\u001b[39;49mimage_size[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    276\u001b[0m     height\u001b[39m=\u001b[39;49mimage_size[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    277\u001b[0m     viewMatrix\u001b[39m=\u001b[39;49mviewm,\n\u001b[1;32m    278\u001b[0m     projectionMatrix\u001b[39m=\u001b[39;49mprojm,\n\u001b[1;32m    279\u001b[0m     shadow\u001b[39m=\u001b[39;49mshadow,\n\u001b[1;32m    280\u001b[0m     flags\u001b[39m=\u001b[39;49mp\u001b[39m.\u001b[39;49mER_SEGMENTATION_MASK_OBJECT_AND_LINKINDEX,\n\u001b[1;32m    281\u001b[0m     renderer\u001b[39m=\u001b[39;49mp\u001b[39m.\u001b[39;49mER_BULLET_HARDWARE_OPENGL)\n\u001b[1;32m    283\u001b[0m \u001b[39m# Get color image.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m color_image_size \u001b[39m=\u001b[39m (image_size[\u001b[39m0\u001b[39m], image_size[\u001b[39m1\u001b[39m], \u001b[39m4\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env.start_rec(\"video\" + str(i+10))\n",
    "\n",
    "    episode, seed = train_dataset.load(i)\n",
    "    task = tasks.names[\"stack-block-pyramid-seq-seen-colors\"]()\n",
    "    task.mode = \"train\"\n",
    "    env.seed(seed)\n",
    "    env.set_task(task)\n",
    "    obs = env.reset()\n",
    "    info = env.info\n",
    "\n",
    "    for _ in range(task.max_steps):\n",
    "        img = train_dataset.get_image(obs)\n",
    "        lang_goal = info['lang_goal']\n",
    "\n",
    "        inp = {'inp_img': img, 'lang_goal': lang_goal}\n",
    "\n",
    "        preds, p0_pix = run_pick(inp)\n",
    "        p0_theta = 0\n",
    "\n",
    "        preds, p1 = run_place(inp, p0_pix)\n",
    "        p1_pix = p1[1:3]\n",
    "        p1_theta = p1_pix[0] * 2 * np.pi / preds.shape[0]\n",
    "\n",
    "        # Pixels to end effector poses.\n",
    "        bounds = np.array([[0.25, 0.75], [-0.5, 0.5], [0, 0.28]])\n",
    "        pix_size = 0.003125\n",
    "\n",
    "        hmap = img[:, :, 3]\n",
    "        p0_xyz = utils.pix_to_xyz(p0_pix, hmap, bounds, pix_size)\n",
    "        p1_xyz = utils.pix_to_xyz(p1_pix, hmap, bounds, pix_size)\n",
    "        p0_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p0_theta))\n",
    "        p1_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p1_theta))\n",
    "\n",
    "        act = {\n",
    "            'pose0': (np.asarray(p0_xyz), np.asarray(p0_xyzw)),\n",
    "            'pose1': (np.asarray(p1_xyz), np.asarray(p1_xyzw)),\n",
    "            'pick': [p0_pix[0], p0_pix[1], p0_theta],\n",
    "            'place': [p1_pix[0], p1_pix[1], p1_theta],\n",
    "        }\n",
    "\n",
    "        print(f'Lang Goal: {lang_goal}')\n",
    "\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    env.end_rec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.end_rec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libx264 @ 0x7492000] -qscale is ignored, -crf is recommended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs = env.reset()\n",
    "info = env.info\n",
    "reward = 0\n",
    "\n",
    "img = train_dataset.get_image(obs)\n",
    "lang_goal = info['lang_goal']\n",
    "\n",
    "inp = {'inp_img': img, 'lang_goal': lang_goal}\n",
    "\n",
    "preds, p0_pix = run_pick(inp)\n",
    "p0_theta = 0\n",
    "\n",
    "preds, p1 = run_place(inp, p0_pix)\n",
    "p1_pix = p1[1:3]\n",
    "p1_theta = p1_pix[0] * 2 * np.pi / preds.shape[0]\n",
    "\n",
    "# Pixels to end effector poses.\n",
    "bounds = np.array([[0.25, 0.75], [-0.5, 0.5], [0, 0.28]])\n",
    "pix_size = 0.003125\n",
    "\n",
    "hmap = img[:, :, 3]\n",
    "p0_xyz = utils.pix_to_xyz(p0_pix, hmap, bounds, pix_size)\n",
    "p1_xyz = utils.pix_to_xyz(p1_pix, hmap, bounds, pix_size)\n",
    "p0_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p0_theta))\n",
    "p1_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p1_theta))\n",
    "\n",
    "act = {\n",
    "    'pose0': (np.asarray(p0_xyz), np.asarray(p0_xyzw)),\n",
    "    'pose1': (np.asarray(p1_xyz), np.asarray(p1_xyzw)),\n",
    "    'pick': [p0_pix[0], p0_pix[1], p0_theta],\n",
    "    'place': [p1_pix[0], p1_pix[1], p1_theta],\n",
    "}\n",
    "\n",
    "obs, reward, done, info = env.step(act)\n",
    "\n",
    "env.end_rec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang Goal: put the green block on the lightest brown block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libx264 @ 0x66bb000] -qscale is ignored, -crf is recommended.\n"
     ]
    }
   ],
   "source": [
    "env.start_rec(\"video\" + str(41))\n",
    "\n",
    "episode, seed = train_dataset.load(i)\n",
    "task = tasks.names[\"stack-block-pyramid-seq-seen-colors\"]()\n",
    "task.mode = \"train\"\n",
    "env.seed(seed)\n",
    "env.set_task(task)\n",
    "obs = env.reset()\n",
    "info = env.info\n",
    "\n",
    "img = train_dataset.get_image(obs)\n",
    "lang_goal = info['lang_goal']\n",
    "\n",
    "inp = {'inp_img': img, 'lang_goal': \"put the brown block on the red block\"}\n",
    "\n",
    "preds, p0_pix = run_pick(inp)\n",
    "p0_theta = 0\n",
    "\n",
    "preds, p1 = run_place(inp, p0_pix)\n",
    "p1_pix = p1[1:3]\n",
    "p1_theta = p1_pix[0] * 2 * np.pi / preds.shape[0]\n",
    "\n",
    "# Pixels to end effector poses.\n",
    "bounds = np.array([[0.25, 0.75], [-0.5, 0.5], [0, 0.28]])\n",
    "pix_size = 0.003125\n",
    "\n",
    "hmap = img[:, :, 3]\n",
    "p0_xyz = utils.pix_to_xyz(p0_pix, hmap, bounds, pix_size)\n",
    "p1_xyz = utils.pix_to_xyz(p1_pix, hmap, bounds, pix_size)\n",
    "p0_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p0_theta))\n",
    "p1_xyzw = utils.eulerXYZ_to_quatXYZW((0, 0, -p1_theta))\n",
    "\n",
    "act = {\n",
    "    'pose0': (np.asarray(p0_xyz), np.asarray(p0_xyzw)),\n",
    "    'pose1': (np.asarray(p1_xyz), np.asarray(p1_xyzw)),\n",
    "    'pick': [p0_pix[0], p0_pix[1], p0_theta],\n",
    "    'place': [p1_pix[0], p1_pix[1], p1_theta],\n",
    "}\n",
    "\n",
    "print(f'Lang Goal: {lang_goal}')\n",
    "\n",
    "obs, reward, done, info = env.step(act)\n",
    "    \n",
    "env.end_rec()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cliport_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "324ebeeaac71c80d764223a438f4ca7776ceeb8ded420916023e4585707f70ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
