{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, dirname, abspath\n",
    "import sys\n",
    "CURRENT_DIR = os.getcwd()\n",
    "sys.path.insert(0, join(CURRENT_DIR, '../..'))  # Import local models\n",
    "\n",
    "from agents.PickPlaceAgent import PickPlaceAgent\n",
    "from cliport.dataset import RavensDataset\n",
    "import torch\n",
    "from src.utils import get_affordance_map_from_formatted_input\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.utils import convert_angle_to_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = PickPlaceAgent(num_rotations=12, lr=1e-4, device='cuda')\n",
    "agent.pick_model.load_state_dict(torch.load(\"/home/ubuntu/VLM/checkpoints/checkpoint_PairPack_epoch390.pth\")['pick_state_dict'])\n",
    "agent.place_model.load_state_dict(torch.load(\"/home/ubuntu/VLM/checkpoints/checkpoint_PairPack_epoch390.pth\")['place_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cfg = {\"dataset\":{\"type\": \"single\",\n",
    "                    \"images\": True,\n",
    "                    \"cache\": False,\n",
    "                    \"augment\":{\"theta_sigma\":60},\n",
    "                    \"cache_size\": 350},\n",
    "                    }\n",
    "\n",
    "# load data\n",
    "train_dataset = RavensDataset('/home/ubuntu/cliport/data/packing-boxes-pairs-full-val', train_dataset_cfg, n_demos=100, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "def show_images(images, affordances):\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "    if not isinstance(affordances, list):\n",
    "        affordances = [affordances]\n",
    "    ncols = min(len(images), len(affordances))\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=ncols, squeeze=False, figsize=(10, 15))\n",
    "    for i in range(ncols):\n",
    "        image = images[i].detach().cpu()\n",
    "        affordance = affordances[i].detach().cpu()\n",
    "\n",
    "        image_uint8 = (image * 255.0).to(torch.uint8)\n",
    "\n",
    "        overlaid_affordance = draw_segmentation_masks(\n",
    "            image_uint8, masks=(affordance > 1.1), colors=\"green\", alpha=1.0\n",
    "        )\n",
    "        overlaid_affordance_pil = torchvision.transforms.functional.to_pil_image(\n",
    "            overlaid_affordance\n",
    "        )\n",
    "\n",
    "        image_pil = torchvision.transforms.functional.to_pil_image(image)\n",
    "        affordance_pil = torchvision.transforms.functional.to_pil_image(affordance)\n",
    "\n",
    "        heatmap = (np.array(plt.cm.jet(affordance_pil)) * 255).astype(np.uint8)\n",
    "        heatmap = heatmap[:, :, 0:3]\n",
    "        heatmap_pil = torchvision.transforms.functional.to_pil_image(heatmap)\n",
    "\n",
    "        blend_pil = Image.blend(image_pil, heatmap_pil, alpha=0.5)\n",
    "\n",
    "        axs[0, i].imshow(np.asarray(overlaid_affordance_pil))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "        label_pil = torchvision.transforms.functional.to_pil_image(affordance)\n",
    "        axs[1, i].imshow(np.asarray(label_pil))\n",
    "        axs[1, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "        axs[2, i].imshow(np.asarray(blend_pil))\n",
    "        axs[2, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataset))\n",
    "inp, _ = batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act, (pick_affordances, place_affordances) = agent.act(inp['img'], inp['lang_goal'])\n",
    "\n",
    "pick_affordances = torch.nn.functional.softmax(pick_affordances, dim=1)\n",
    "pick_affordances = pick_affordances.detach().cpu()\n",
    "pick_affordances = pick_affordances.view(320,160)\n",
    "\n",
    "place_affordances = torch.nn.functional.softmax(place_affordances, dim=1)\n",
    "place_affordances = place_affordances.detach().cpu()\n",
    "place_affordances = place_affordances.view(12,320,160)\n",
    "\n",
    "print(act) \n",
    "print(inp['lang_goal'])\n",
    "show_images(torch.tensor(inp['img'][:,:,0:3].transpose((2, 1,0))/255), pick_affordances.T/pick_affordances.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the language command\n",
    "new_lang_goal = \"Insert the green squares into that brown box\"\n",
    "act, (pick_affordances, place_affordances) = agent.act(inp['img'], new_lang_goal)\n",
    "\n",
    "pick_affordances = torch.nn.functional.softmax(pick_affordances, dim=1)\n",
    "pick_affordances = pick_affordances.detach().cpu()\n",
    "pick_affordances = pick_affordances.view(320,160)\n",
    "\n",
    "place_affordances = torch.nn.functional.softmax(place_affordances, dim=1)\n",
    "place_affordances = place_affordances.detach().cpu()\n",
    "place_affordances = place_affordances.view(12,320,160)\n",
    "\n",
    "print(act) \n",
    "print(new_lang_goal)\n",
    "show_images(torch.tensor(inp['img'][:,:,0:3].transpose((2, 1,0))/255), pick_affordances.T/pick_affordances.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataset))\n",
    "inp, _ = batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act, (pick_affordances, place_affordances) = agent.act(inp['img'], inp['lang_goal'])\n",
    "\n",
    "pick_affordances = torch.nn.functional.softmax(pick_affordances, dim=1)\n",
    "pick_affordances = pick_affordances.detach().cpu()\n",
    "pick_affordances = pick_affordances.view(320,160)\n",
    "\n",
    "place_affordances = torch.nn.functional.softmax(place_affordances, dim=1)\n",
    "place_affordances = place_affordances.detach().cpu()\n",
    "place_affordances = place_affordances.view(12,320,160)\n",
    "\n",
    "best_place_rotation_idx = convert_angle_to_channel(act['place'][2], 12)\n",
    "place_affordances = place_affordances[best_place_rotation_idx]\n",
    "\n",
    "print(act) \n",
    "print(inp['lang_goal'])\n",
    "show_images(torch.tensor(inp['img'][:,:,0:3].transpose((2, 1,0))/255), place_affordances.T/place_affordances.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the language command\n",
    "new_lang_goal = \"Insert the pink looking square into that brown box\"\n",
    "act, (pick_affordances, place_affordances) = agent.act(inp['img'], new_lang_goal)\n",
    "\n",
    "pick_affordances = torch.nn.functional.softmax(pick_affordances, dim=1)\n",
    "pick_affordances = pick_affordances.detach().cpu()\n",
    "pick_affordances = pick_affordances.view(320,160)\n",
    "\n",
    "place_affordances = torch.nn.functional.softmax(place_affordances, dim=1)\n",
    "place_affordances = place_affordances.detach().cpu()\n",
    "place_affordances = place_affordances.view(12,320,160)\n",
    "\n",
    "print(act) \n",
    "print(new_lang_goal)\n",
    "show_images(torch.tensor(inp['img'][:,:,0:3].transpose((2, 1,0))/255), pick_affordances.T/pick_affordances.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cliport_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "324ebeeaac71c80d764223a438f4ca7776ceeb8ded420916023e4585707f70ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
